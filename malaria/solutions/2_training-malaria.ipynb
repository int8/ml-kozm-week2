{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00b4d77e-7938-4599-b2cc-5a6d4584e8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm \n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "\n",
    "TWOJ_KOD = None \n",
    "\n",
    "def create_bbox_coords(bbox):\n",
    "    xmin = float(bbox.find('xmin').text)\n",
    "    ymin = float(bbox.find('ymin').text)\n",
    "    xmax = float(bbox.find('xmax').text)\n",
    "    ymax = float(bbox.find('ymax').text)\n",
    "    return [xmin, ymin, xmax, ymax]\n",
    "\n",
    "def create_mask(plasmodium_img, bbox):\n",
    "    xmin, ymin, xmax, ymax = create_bbox_coords(bbox)\n",
    "    mask = np.zeros((plasmodium_img.size[1], plasmodium_img.size[0]), dtype=np.uint8)\n",
    "    mask[int(ymin):int(ymax), int(xmin):int(xmax)] = 1 \n",
    "    return mask \n",
    "\n",
    "    \n",
    "class MalariaPlasmodiumDataset(torch.utils.data.Dataset):\n",
    "    # Będziemy czytać pliki jpg i odpowiadające im pliki XML \n",
    "    # z katalogu directory_root \n",
    "    # Podamy też transformacje jakie chcemy przeprowadzać na zwracanych wartościach \n",
    "    \n",
    "    def __init__(self, directory_root, images_transforms=None):\n",
    "\n",
    "        # Przypisujemy parametetry konstruktora do self \n",
    "        # Chcemy aby nasz przyszły obiekt wiedzial o tym gdzie szukać plików oraz \n",
    "        # jakie transformacje wykonywać na przeczytanych JPG \n",
    "        self.directory_root = directory_root        \n",
    "        self.images_transforms = images_transforms\n",
    "\n",
    "        # Listujemy wszystkie pliki które mają rozszerzenie \"JPG\" \n",
    "        self.all_image_files = sorted([img for img in os.listdir(directory_root) if img.endswith(\".jpg\")])\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        # \"magiczna\" metoda __getitem__ jest wykorzystywana kiedy chcemy aby nasz obiekt był dostępny poprzez operator [int] \n",
    "        # podobnie jak lista czy dict \n",
    "        single_plasmodium_img_path = self.get_single_plasmodium_path(idx)\n",
    "        single_annotation_file_path = single_plasmodium_img_path.replace(\".jpg\", \".xml\")\n",
    "        plasmodium_img = Image.open(single_plasmodium_img_path).convert(\"RGB\") \n",
    "        \n",
    "        # czytamy xml file\n",
    "        annotations = ET.parse(single_annotation_file_path)\n",
    "        boxes = []\n",
    "        masks = []        \n",
    "        \n",
    "        for detected_plasmodium in annotations.findall('object'):            \n",
    "            bbox = detected_plasmodium.find('bndbox')\n",
    "            # dodajemy bboxes\n",
    "            boxes.append(\n",
    "                create_bbox_coords(bbox)\n",
    "            )\n",
    "            # dodajemy maski \n",
    "            masks.append(\n",
    "                create_mask(\n",
    "                    plasmodium_img, bbox\n",
    "                )\n",
    "            )\n",
    "        image_id = torch.tensor([idx])\n",
    "        if boxes:\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            masks = torch.as_tensor(np.array(masks), dtype=torch.uint8)  \n",
    "            labels = torch.ones((len(boxes),), dtype=torch.int64) \n",
    "        else:\n",
    "            boxes =  torch.empty(0, 4)\n",
    "            masks = torch.zeros(0, plasmodium_img.height, plasmodium_img.width, dtype=torch.uint8)\n",
    "            labels = torch.zeros(0, dtype=torch.int64)\n",
    "        \n",
    "        if self.images_transforms is not None:\n",
    "            transformed_plasmodium_img = self.images_transforms(plasmodium_img)\n",
    "        else:\n",
    "            transformed_plasmodium_img = plasmodium_img            \n",
    "        # zapisujemy target dla jednego pliku img \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"masks\"] = masks\n",
    "        \n",
    "        return transformed_plasmodium_img, target\n",
    "\n",
    "    def get_single_plasmodium_path(self, idx):\n",
    "        single_plasmodium_img_path = os.path.join(self.directory_root, self.all_image_files[idx])\n",
    "        return single_plasmodium_img_path\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        # magiczna metoda __len__ jest używana gdy na instancji wykonujemy len() \n",
    "        return len(self.all_image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "009151ea-5df0-4417-9dd3-405f8cfac13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def draw_bounding_boxes(image_path, bboxes, scores=None, color=(255, 0, 0), return_pt = False):    \n",
    "    img_pil = Image.open(image_path).convert(\"RGBA\")\n",
    "    new = Image.new('RGBA', img_pil.size, (255, 255, 255, 0))\n",
    "    draw = ImageDraw.Draw(new)\n",
    "\n",
    "    for i, box in enumerate(bboxes):\n",
    "        xmin, ymin, xmax, ymax = box        \n",
    "        if scores is not None:          \n",
    "            alpha = int(255 * scores[i])  # Convert score to an alpha value.                      \n",
    "            color_with_alpha = color + (alpha,)\n",
    "        else:       \n",
    "            color_with_alpha = color + (255,)\n",
    "        draw.rectangle([xmin, ymin, xmax, ymax], outline=color_with_alpha, width=2)\n",
    "\n",
    "    out = Image.alpha_composite(img_pil, new).convert(\"RGB\")\n",
    "    return T.ToTensor()(out) if return_pt else out \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9dc4317b-42d2-4625-9468-4d09629ed67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    # https://stackoverflow.com/questions/58151507/why-pytorch-officially-use-mean-0-485-0-456-0-406-and-std-0-229-0-224-0-2\n",
    "])\n",
    "\n",
    "dataset = MalariaPlasmodiumDataset(\n",
    "    \"plasmodium-phonecamera/train/\", images_transforms=images_transforms\n",
    ")\n",
    "\n",
    "# TODO: podzielmy sobie nasz dataset na 2 randomowe rozłączne subsety w proporcjach 0.85,0.15\n",
    "# torch.utils.data.random_split <= nasz przyjaciel \n",
    "train_set, val_set =  torch.utils.data.random_split(dataset, [0.85, 0.15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dc0357b-6e9c-43b4-819c-9ec39de75a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(855, 150)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set), len(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af38b4ba-f84f-4985-96bc-57462ec06ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=1, \n",
    "    shuffle=True, num_workers=2,\n",
    "    collate_fn=lambda x: tuple(zip(*x))\n",
    ")\n",
    "\n",
    "\n",
    "val_data_loader = torch.utils.data.DataLoader(\n",
    "    val_set, batch_size=1, \n",
    "    shuffle=True, num_workers=2,\n",
    "    collate_fn=lambda x: tuple(zip(*x))\n",
    ")\n",
    "\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2()\n",
    "\n",
    "num_classes = 2  # 1 zarodziec + tło\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "hidden_layer = 256\n",
    "model.roi_heads.mask_predictor = torchvision.models.detection.mask_rcnn.MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "012b325c-8fa8-485d-8aa2-3c094e3ff5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   2%|██▌                                                                                                                                                                   | 13/855 [01:13<1:19:49,  5.69s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m     25\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 26\u001b[0m     \u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# evaluation\u001b[39;00m\n",
      "File \u001b[0;32m~/work/hobby/ml-kozm-week2/mlcourse/lib/python3.8/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/hobby/ml-kozm-week2/mlcourse/lib/python3.8/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# Parameters\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "# only some parameters are trainable \n",
    "optimizer = torch.optim.Adam(params, lr=0.0005) # TODO: otwórz optimizer Adam (lr=0.0005)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# TODO: initialize best_eval_metric_result \n",
    "best_eval_metric_result = 0.0\n",
    "\n",
    "num_epochs = 60\n",
    "for epoch in range(num_epochs):\n",
    "    # train \n",
    "    model.train() \n",
    "    for images, targets in tqdm(train_data_loader, desc=f\"Training epoch {epoch}\"):     \n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # evaluation\n",
    "    model.eval()    \n",
    "    eval_metric = MeanAveragePrecision(iou_type=\"bbox\", iou_thresholds=[0.5]) #TODO utwórz mAP metric z torchmetrics [iou_type=\"bbox\", iou_thresholds = [0.5]]\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(val_data_loader, desc=\"Evaluation...\"):        \n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            output = model(images)\n",
    "    \n",
    "            # MAP metric update \n",
    "            eval_metric.update(output, targets)    \n",
    "        # metryka po całej epoce \n",
    "        result = eval_metric.compute()            \n",
    "        writer.add_scalar(\"map@validation_set\", result['map'].detach().numpy(), epoch)\n",
    "        # Zapiszmy obecnie najlepszy model \n",
    "        if result['map'].detach().numpy() > best_eval_metric_result:           \n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            best_eval_metric_result = result['map'].detach().numpy()        \n",
    "\n",
    "\n",
    "    # Po wyjściu z pętli walidacji powyżej zmienne targets i outputs nadal istnieją - skorzystamy z nich \n",
    "    # by wyświetlić jak wyglądają przykładowe detekcje po tej epoce \n",
    "    \n",
    "    bboxes_true = targets[0]['boxes']\n",
    "    bboxes_predicted = output[0]['boxes']\n",
    "    scores = output[0]['scores']\n",
    "    img_id = targets[0]['image_id']\n",
    "    img = val_set.dataset.get_single_plasmodium_path(targets[0]['image_id'])\n",
    "\n",
    "    # zapiszmy zdjecie z predykcjami obok prawdziwych zarodźców w tensorboard \n",
    "    img_tensor = torch.cat([\n",
    "        draw_bounding_boxes(img, bboxes_true, return_pt=True), \n",
    "        draw_bounding_boxes(img, bboxes_predicted, scores, color=(0,255,0), return_pt=True)\n",
    "        ], dim=2\n",
    "    )\n",
    "    # TODO: add_image to tensorboard to see current detections \n",
    "    writer.add_image(\"predictions-vs-true\", img_tensor, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d080d6f8-6c3c-454e-859a-920bd973c70f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mSummaryWriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mwalltime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdataformats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'CHW'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Add image data to summary.\n",
       "\n",
       "Note that this requires the ``pillow`` package.\n",
       "\n",
       "Args:\n",
       "    tag (str): Data identifier\n",
       "    img_tensor (torch.Tensor, numpy.ndarray, or string/blobname): Image data\n",
       "    global_step (int): Global step value to record\n",
       "    walltime (float): Optional override default walltime (time.time())\n",
       "      seconds after epoch of event\n",
       "    dataformats (str): Image data format specification of the form\n",
       "      CHW, HWC, HW, WH, etc.\n",
       "Shape:\n",
       "    img_tensor: Default is :math:`(3, H, W)`. You can use ``torchvision.utils.make_grid()`` to\n",
       "    convert a batch of tensor into 3xHxW format or call ``add_images`` and let us do the job.\n",
       "    Tensor with :math:`(1, H, W)`, :math:`(H, W)`, :math:`(H, W, 3)` is also suitable as long as\n",
       "    corresponding ``dataformats`` argument is passed, e.g. ``CHW``, ``HWC``, ``HW``.\n",
       "\n",
       "Examples::\n",
       "\n",
       "    from torch.utils.tensorboard import SummaryWriter\n",
       "    import numpy as np\n",
       "    img = np.zeros((3, 100, 100))\n",
       "    img[0] = np.arange(0, 10000).reshape(100, 100) / 10000\n",
       "    img[1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000\n",
       "\n",
       "    img_HWC = np.zeros((100, 100, 3))\n",
       "    img_HWC[:, :, 0] = np.arange(0, 10000).reshape(100, 100) / 10000\n",
       "    img_HWC[:, :, 1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000\n",
       "\n",
       "    writer = SummaryWriter()\n",
       "    writer.add_image('my_image', img, 0)\n",
       "\n",
       "    # If you have non-default dimension setting, set the dataformats argument.\n",
       "    writer.add_image('my_image_HWC', img_HWC, 0, dataformats='HWC')\n",
       "    writer.close()\n",
       "\n",
       "Expected result:\n",
       "\n",
       ".. image:: _static/img/tensorboard/add_image.png\n",
       "   :scale: 50 %\n",
       "\u001b[0;31mFile:\u001b[0m      ~/work/hobby/ml-kozm-week2/mlcourse/lib/python3.8/site-packages/torch/utils/tensorboard/writer.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?SummaryWriter.add_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e75fcc-312c-49d7-b3f5-4c512df3c29f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
