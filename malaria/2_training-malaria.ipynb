{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00b4d77e-7938-4599-b2cc-5a6d4584e8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm \n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "\n",
    "TWOJ_KOD = None \n",
    "\n",
    "def create_bbox_coords(bbox):\n",
    "    xmin = float(bbox.find('xmin').text)\n",
    "    ymin = float(bbox.find('ymin').text)\n",
    "    xmax = float(bbox.find('xmax').text)\n",
    "    ymax = float(bbox.find('ymax').text)\n",
    "    return [xmin, ymin, xmax, ymax]\n",
    "\n",
    "def create_mask(plasmodium_img, bbox):\n",
    "    xmin, ymin, xmax, ymax = create_bbox_coords(bbox)\n",
    "    mask = np.zeros((plasmodium_img.size[1], plasmodium_img.size[0]), dtype=np.uint8)\n",
    "    mask[int(ymin):int(ymax), int(xmin):int(xmax)] = 1 \n",
    "    return mask \n",
    "\n",
    "    \n",
    "class MalariaPlasmodiumDataset(torch.utils.data.Dataset):\n",
    "    # Będziemy czytać pliki jpg i odpowiadające im pliki XML \n",
    "    # z katalogu directory_root \n",
    "    # Podamy też transformacje jakie chcemy przeprowadzać na zwracanych wartościach \n",
    "    \n",
    "    def __init__(self, directory_root, images_transforms=None):\n",
    "\n",
    "        # Przypisujemy parametetry konstruktora do self \n",
    "        # Chcemy aby nasz przyszły obiekt wiedzial o tym gdzie szukać plików oraz \n",
    "        # jakie transformacje wykonywać na przeczytanych JPG \n",
    "        self.directory_root = directory_root        \n",
    "        self.images_transforms = images_transforms\n",
    "\n",
    "        # Listujemy wszystkie pliki które mają rozszerzenie \"JPG\" \n",
    "        self.all_image_files = sorted([img for img in os.listdir(directory_root) if img.endswith(\".jpg\")])\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        # \"magiczna\" metoda __getitem__ jest wykorzystywana kiedy chcemy aby nasz obiekt był dostępny poprzez operator [int] \n",
    "        # podobnie jak lista czy dict \n",
    "        single_plasmodium_img_path = self.get_single_plasmodium_path(idx)\n",
    "        single_annotation_file_path = single_plasmodium_img_path.replace(\".jpg\", \".xml\")\n",
    "        plasmodium_img = Image.open(single_plasmodium_img_path).convert(\"RGB\") \n",
    "        \n",
    "        # czytamy xml file\n",
    "        annotations = ET.parse(single_annotation_file_path)\n",
    "        boxes = []\n",
    "        masks = []        \n",
    "        \n",
    "        for detected_plasmodium in annotations.findall('object'):            \n",
    "            bbox = detected_plasmodium.find('bndbox')\n",
    "            # dodajemy bboxes\n",
    "            boxes.append(\n",
    "                create_bbox_coords(bbox)\n",
    "            )\n",
    "            # dodajemy maski \n",
    "            masks.append(\n",
    "                create_mask(\n",
    "                    plasmodium_img, bbox\n",
    "                )\n",
    "            )\n",
    "        image_id = torch.tensor([idx])\n",
    "        if boxes:\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            masks = torch.as_tensor(np.array(masks), dtype=torch.uint8)  \n",
    "            labels = torch.ones((len(boxes),), dtype=torch.int64) \n",
    "        else:\n",
    "            boxes =  torch.empty(0, 4)\n",
    "            masks = torch.zeros(0, plasmodium_img.height, plasmodium_img.width, dtype=torch.uint8)\n",
    "            labels = torch.zeros(0, dtype=torch.int64)\n",
    "        \n",
    "        if self.images_transforms is not None:\n",
    "            transformed_plasmodium_img = self.images_transforms(plasmodium_img)\n",
    "        else:\n",
    "            transformed_plasmodium_img = plasmodium_img            \n",
    "        # zapisujemy target dla jednego pliku img \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"masks\"] = masks\n",
    "        \n",
    "        return transformed_plasmodium_img, target\n",
    "\n",
    "    def get_single_plasmodium_path(self, idx):\n",
    "        single_plasmodium_img_path = os.path.join(self.directory_root, self.all_image_files[idx])\n",
    "        return single_plasmodium_img_path\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        # magiczna metoda __len__ jest używana gdy na instancji wykonujemy len() \n",
    "        return len(self.all_image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "009151ea-5df0-4417-9dd3-405f8cfac13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def draw_bounding_boxes(image_path, bboxes, scores=None, color=(255, 0, 0), return_pt = False):    \n",
    "    img_pil = Image.open(image_path).convert(\"RGBA\")\n",
    "    new = Image.new('RGBA', img_pil.size, (255, 255, 255, 0))\n",
    "    draw = ImageDraw.Draw(new)\n",
    "\n",
    "    for i, box in enumerate(bboxes):\n",
    "        xmin, ymin, xmax, ymax = box        \n",
    "        if scores is not None:          \n",
    "            alpha = int(255 * scores[i])  # Convert score to an alpha value.                      \n",
    "            color_with_alpha = color + (alpha,)\n",
    "        else:       \n",
    "            color_with_alpha = color + (255,)\n",
    "        draw.rectangle([xmin, ymin, xmax, ymax], outline=color_with_alpha, width=2)\n",
    "\n",
    "    out = Image.alpha_composite(img_pil, new).convert(\"RGB\")\n",
    "    return T.ToTensor()(out) if return_pt else out \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dc4317b-42d2-4625-9468-4d09629ed67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(), # chcemy najpierw \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    # https://stackoverflow.com/questions/58151507/why-pytorch-officially-use-mean-0-485-0-456-0-406-and-std-0-229-0-224-0-2\n",
    "])\n",
    "\n",
    "dataset = MalariaPlasmodiumDataset(\n",
    "    \"/home/kamil/Downloads/plasmodium-phonecamera/train/\", images_transforms=images_transforms\n",
    ")\n",
    "\n",
    "# TODO: podzielmy sobie nasz dataset na 3 randomowe rozłączne subsety w proporcjach 0.8,0.1\n",
    "# torch.utils.data.random_split <= nasz przyjaciel \n",
    "train_set, val_set = TWOJ_KOD, TWOJ_KOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af38b4ba-f84f-4985-96bc-57462ec06ef9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdetection\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfaster_rcnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastRCNNPredictor\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdetection\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmask_rcnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MaskRCNNPredictor\n\u001b[0;32m----> 6\u001b[0m train_data_loader \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m val_data_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m     14\u001b[0m     val_set, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \n\u001b[1;32m     15\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     16\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mx))\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mdetection\u001b[38;5;241m.\u001b[39mmaskrcnn_resnet50_fpn_v2()\n",
      "File \u001b[0;32m~/work/hobby/ml-kozm-week2/mlcourse/lib/python3.8/site-packages/torch/utils/data/dataloader.py:350\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[0;32m--> 350\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/work/hobby/ml-kozm-week2/mlcourse/lib/python3.8/site-packages/torch/utils/data/sampler.py:142\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement, \u001b[38;5;28mbool\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_samples\u001b[49m, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/work/hobby/ml-kozm-week2/mlcourse/lib/python3.8/site-packages/torch/utils/data/sampler.py:149\u001b[0m, in \u001b[0;36mRandomSampler.num_samples\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnum_samples\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;66;03m# dataset size might change at runtime\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_samples \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_source\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_samples\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=1, \n",
    "    shuffle=True, num_workers=2,\n",
    "    collate_fn=lambda x: tuple(zip(*x))\n",
    ")\n",
    "\n",
    "\n",
    "val_data_loader = torch.utils.data.DataLoader(\n",
    "    val_set, batch_size=1, \n",
    "    shuffle=True, num_workers=2,\n",
    "    collate_fn=lambda x: tuple(zip(*x))\n",
    ")\n",
    "\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2()\n",
    "\n",
    "num_classes = 2  # 1 zarodziec + tło\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "hidden_layer = 256\n",
    "model.roi_heads.mask_predictor = torchvision.models.detection.mask_rcnn.MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012b325c-8fa8-485d-8aa2-3c094e3ff5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# Parameters\n",
    "params = [p for p in model.parameters() if p.requires_grad] # only some parameters are trainable \n",
    "optimizer = TWOJ_KOD # TODO: otwórz optimizer Adam (lr=0.0005)\n",
    "\n",
    "writer = SummaryWriter() \n",
    "\n",
    "\n",
    "# TODO: initialize best_eval_metric_result \n",
    "best_eval_metric_result = TWOJ_KOD\n",
    "\n",
    "num_epochs = 60\n",
    "for epoch in range(num_epochs):\n",
    "    # train \n",
    "    model.train() \n",
    "    for images, targets in tqdm(train_data_loader, desc=f\"Training epoch {epoch}\"):     \n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # evaluation\n",
    "    model.eval()    \n",
    "    eval_metric = TWOJ_KOD #TODO utwórz mAP metric z tensormetrics  [iou_type=\"bbox\", iou_thresholds = [0.5]]\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(val_data_loader, desc=\"Evaluation...\"):        \n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            output = model(images)\n",
    "    \n",
    "            # MAP metric update \n",
    "            eval_metric.update(output, targets)    \n",
    "        # metryka po całej epoce \n",
    "        result = eval_metric.compute()            \n",
    "        writer.add_scalar(\"map@validation_set\", result['map'].detach().numpy(), epoch)\n",
    "        # Zapiszmy obecnie najlepszy model \n",
    "        if result['map'].detach().numpy() > best_eval_metric_result:           \n",
    "            pass \n",
    "            # TODO: zapis model.state_dict() jako 'best_model.pth'\n",
    "            # TODO: nadpisz best_eval_metric_result             \n",
    "            \n",
    "\n",
    "    # Po wyjściu z pętli walidacji powyżej zmienne targets i outputs nadal istnieją - skorzystamy z nich \n",
    "    # by wyświetlić jak wyglądają przykładowe detekcje po tej epoce \n",
    "    \n",
    "    bboxes_true = targets[0]['boxes']\n",
    "    bboxes_predicted = output[0]['boxes']\n",
    "    scores = output[0]['scores']\n",
    "    img_id = targets[0]['image_id']\n",
    "    img = val_set.dataset.get_single_plasmodium_path(targets[0]['image_id'])\n",
    "\n",
    "    # zapiszmy zdjecie z predykcjami obok prawdziwych zarodźców w tensorboard \n",
    "    img_tensor = torch.cat([\n",
    "        draw_bounding_boxes(img, bboxes_true, return_pt=True), \n",
    "        draw_bounding_boxes(img, bboxes_predicted, scores, color=(0,255,0), return_pt=True)\n",
    "        ], dim=2\n",
    "    )\n",
    "    # TODO: add_image to tensorboard to see current detections \n",
    "    TWOJ_KOD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d080d6f8-6c3c-454e-859a-920bd973c70f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
